{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following \"Pytorch Computer Vision Cookbook\" tutorial on video classification\n",
    "https://github.com/PacktPublishing/PyTorch-Computer-Vision-Cookbook/tree/master/Chapter10 <br> a.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = \"C:\\\\Users\\\\JUNHAO.KOH\\\\Desktop\\\\Video_data\"\n",
    "folder    = \"hmdb51_org\"\n",
    "folder_jpg = \"hmdb51_jpg\"\n",
    "path2folder = os.path.join(path2data,folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def preprocess_videos(root_dir, folder, dataset_name = \"hmdb51\"):\n",
    "    extension = \".avi\"\n",
    "    n_frames = 16\n",
    "    data_train_dir = os.path.join(root_dir, dataset_name)\n",
    "    if not os.path.exists(os.path.join(root_dir,dataset_name)):\n",
    "        data_train_dir = os.path.join(root_dir, dataset_name)\n",
    "        os.mkdir(data_train_dir)\n",
    "        os.mkdir(os.path.join(data_train_dir, \"train\"))\n",
    "        os.mkdir(os.path.join(data_train_dir, \"val\"))\n",
    "        os.mkdir(os.path.join(data_train_dir, \"test\"))\n",
    "    \n",
    "    target_folder_dir = os.path.join(root_dir, folder)\n",
    "    classes = sorted(os.listdir(target_folder_dir))\n",
    "\n",
    "    for class_names in classes:\n",
    "\n",
    "        class_videos = os.listdir(os.path.join(target_folder_dir, class_names))\n",
    "        train_val_videos, test_videos = train_test_split(class_videos, test_size=0.2, random_state=42)\n",
    "        train_videos, val_videos = train_test_split(train_val_videos, test_size=0.2, random_state=42)\n",
    "        \n",
    "        for train_vid_names in train_videos:\n",
    "            if extension not in train_vid_names:\n",
    "                continue\n",
    "            path2vid = os.path.join(target_folder_dir, class_names, train_vid_names)\n",
    "            frames, vlen = get_frames(path2vid, n_frames = n_frames)\n",
    "            vid_name = path2vid.replace(extension, \"\")\n",
    "            vid_name = vid_name.split(\"\\\\\")[-1]\n",
    "            path2store = os.path.join(data_train_dir, \"train\", class_names, vid_name )\n",
    "            if not os.path.exists(os.path.join(data_train_dir, \"train\", class_names)):\n",
    "                os.mkdir(os.path.join(data_train_dir, \"train\", class_names))\n",
    "            if not os.path.exists(path2store):\n",
    "                os.mkdir(path2store)\n",
    "            store_frames(frames, path2store)\n",
    "            \n",
    "        for val_vid_names in val_videos:\n",
    "            if extension not in val_vid_names:\n",
    "                continue\n",
    "            path2vid = os.path.join(target_folder_dir, class_names, val_vid_names)\n",
    "            frames, vlen = get_frames(path2vid, n_frames = n_frames)\n",
    "            vid_name = path2vid.replace(extension, \"\")\n",
    "            vid_name = vid_name.split(\"\\\\\")[-1]\n",
    "            path2store = os.path.join(data_train_dir, \"val\", class_names, vid_name )\n",
    "            if not os.path.exists(os.path.join(data_train_dir, \"val\", class_names)):\n",
    "                os.mkdir(os.path.join(data_train_dir, \"val\", class_names))\n",
    "            if not os.path.exists(path2store):\n",
    "                os.mkdir(path2store)\n",
    "            store_frames(frames, path2store)\n",
    "            \n",
    "        for test_vid_names in test_videos:\n",
    "            if extension not in test_vid_names:\n",
    "                continue\n",
    "            path2vid = os.path.join(target_folder_dir, class_names, test_vid_names)\n",
    "            frames, vlen = get_frames(path2vid, n_frames = n_frames)\n",
    "            vid_name = path2vid.replace(extension, \"\")\n",
    "            vid_name = vid_name.split(\"\\\\\")[-1]\n",
    "            path2store = os.path.join(data_train_dir, \"test\", class_names, vid_name )\n",
    "            if not os.path.exists(os.path.join(data_train_dir, \"test\", class_names)):\n",
    "                os.mkdir(os.path.join(data_train_dir, \"test\", class_names))            \n",
    "            if not os.path.exists(path2store):\n",
    "                os.mkdir(path2store)\n",
    "            store_frames(frames, path2store)            \n",
    "\n",
    "\n",
    "def store_frames(frames, path2store):\n",
    "    for ii, frame in enumerate(frames):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  \n",
    "        path2img = os.path.join(path2store, \"frame\"+str(ii)+\".jpg\")\n",
    "        cv2.imwrite(path2img, frame)       \n",
    "        \n",
    "def get_frames(filename, n_frames= 1):\n",
    "    frames = []\n",
    "    v_cap = cv2.VideoCapture(filename)\n",
    "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_list= np.linspace(0, v_len-1, n_frames+1, dtype=np.int16)\n",
    "    \n",
    "    for fn in range(v_len):\n",
    "        success, frame = v_cap.read()\n",
    "        if success is False:\n",
    "            continue\n",
    "        if (fn in frame_list):\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  \n",
    "            frames.append(frame)\n",
    "    v_cap.release()\n",
    "    return frames, v_len    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_videos(path2data, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos(root_dir, target_folder):\n",
    "    target_folder_dir = os.path.join(root_dir, target_folder)\n",
    "    classes = sorted(os.listdir(target_folder_dir))\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for cat in classes:\n",
    "\n",
    "        cat_dir = os.path.join(target_folder_dir, cat)\n",
    "        video_path = [os.path.join(cat_dir, loc) for loc in sorted(os.listdir(cat_dir))]\n",
    "        ids.extend(video_path)\n",
    "        labels.extend([cat]*len(video_path))\n",
    "    return ids, labels, classes\n",
    "def _find_classes(root_dir):\n",
    "    classes = [d.name for d in os.scandir(root_dir) if d.is_dir()]\n",
    "    classes.sort()\n",
    "    class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, class_to_idx = _find_classes(os.path.join(path2data,folder_jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, labels ,classes = get_videos(path2data, folder_jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_ids, test_ids, train_val_labels,  test_labels = train_test_split(ids, labels, test_size=0.2, random_state=42)\n",
    "train_ids, val_ids, train_labels,  val_labels = train_test_split(train_val_ids,train_val_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self,ids, labels,labels_dict, transform=None):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.labels_dict = labels_dict\n",
    "        self.transform = transform\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        frames = []\n",
    "        if self.transform:\n",
    "            for frame_name in os.listdir(self.ids[idx])[:16]:\n",
    "                img = Image.open(os.path.join(self.ids[idx], frame_name))\n",
    "                img = self.transform(img)\n",
    "                frames.append(img)\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for frame_name in os.listdir(self.ids[idx])[:16]:\n",
    "                img = Image.open(os.path.join(self.ids[idx], frame_name))\n",
    "                frames.append(img)\n",
    "        \n",
    "        label= self.labels_dict[self.labels[idx]]\n",
    "        frames_tr = torch.stack(frames)\n",
    "        return frames_tr, label\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((112,112), interpolation=3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((112,112), interpolation=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = VideoDataset(train_ids, train_labels, class_to_idx,transform = data_transform[\"train\"])\n",
    "val_ds = VideoDataset(val_ids, val_labels, class_to_idx,transform = data_transform[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_r3d_18(batch):\n",
    "    imgs_batch, label_batch = list(zip(*batch))\n",
    "    imgs_batch = [imgs for imgs in imgs_batch if len(imgs)>0]\n",
    "    label_batch = [torch.tensor(l) for l, imgs in zip(label_batch, imgs_batch) if len(imgs)>0]\n",
    "    imgs_tensor = torch.stack(imgs_batch)\n",
    "    imgs_tensor = torch.transpose(imgs_tensor, 2, 1)\n",
    "    labels_tensor = torch.stack(label_batch)\n",
    "    return imgs_tensor,labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size= 16, \n",
    "                      shuffle=True, collate_fn = collate_fn_r3d_18)\n",
    "val_dl = DataLoader(val_ds, batch_size= 2*16, \n",
    "                     shuffle=False, collate_fn = collate_fn_r3d_18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(classes)\n",
    "model = models.video.r3d_18(pretrained=True, progress=False)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = {\"train\": train_ds,\n",
    "             \"val\": val_ds}\n",
    "data_loader = {\"train\": train_dl,\n",
    "              \"val\":val_dl}\n",
    "dataset_sizes = {x: len(video_data[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 4329, 'val': 1083}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.0309 Acc: 0.5318\n",
      "val Loss: 1.6530 Acc: 0.5743\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.4849 Acc: 0.6387\n",
      "val Loss: 1.3981 Acc: 0.6214\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-660926e62476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-97cdbaeaf200>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;31m# statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, loss_history, metric_history = train_model(model, criterion, optimizer, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs = 25):\n",
    "    since = time.time()\n",
    "    loss_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    metric_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in data_loader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            loss_history[phase].append(epoch_loss)\n",
    "            metric_history[phase].append(1.0 - epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

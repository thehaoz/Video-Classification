{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Adapting some code snippets from \"Pytorch Computer Vision Cookbook\" tutorial on video classification</i> [link](https://github.com/PacktPublishing/PyTorch-Computer-Vision-Cookbook/tree/master/Chapter10)\n",
    "\n",
    "# Training Pipeline\n",
    "\n",
    "<i>Important directory to note: </i>\n",
    "\n",
    "1) <i>Main Folder </i><br>\n",
    "2) <i>Folder containing videos </i> <br>\n",
    "3) <i>Folder containing video frames </i>\n",
    "\n",
    "For the training pipeline, root directory will be the folder containing <i>train, val and test </i> folders.\n",
    "\n",
    "e.g\n",
    "```\n",
    "hmdb51/\n",
    "├── test/\n",
    "│   ├── brush_hair/\n",
    "│   │   ├── April_09_brush_hair_u_nm_np1_ba_goo_0/\n",
    "│   │   │   ├── frame0.jpg\n",
    "│   │   │   ├── frame1.jpg\n",
    "│   │   │   ├── frame10.jpg\n",
    "|   |   |   ......\n",
    "|   |   |...\n",
    "│   ├── cartwheel/\n",
    "│   │   ├── (Rad)Schlag_die_Bank!_cartwheel_f_cm_np1_le_med_0/\n",
    "│   │   │   ├── frame0.jpg\n",
    "|   |...\n",
    "├── train/\n",
    "│   ├── brush_hair/\n",
    "│   │   ├── April_09_brush_hair_u_nm_np1_ba_goo_2/\n",
    "│   │   │   ├── frame0.jpg\n",
    "│   │   │   ├── frame1.jpg\n",
    "│   │   │   ├── frame10.jpg\n",
    "|   |   |   |...\n",
    "|   |   |...\n",
    "|   |...\n",
    "└── val/\n",
    "    ├── brush_hair/\n",
    "    │   ├── April_09_brush_hair_u_nm_np1_ba_goo_1/\n",
    "    │   │   ├── frame0.jpg\n",
    "    │   │   ├── frame1.jpg\n",
    "    │   │   ├── frame10.jpg\n",
    "    │   ├── Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_1/\n",
    "    │   │   ├── frame0.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = \"C:\\\\Users\\\\USER\\\\Desktop\\\\Video_data\"\n",
    "folder    = \"hmdb51_org\"\n",
    "folder_jpg = \"hmdb51_jpg\"\n",
    "path2folder = os.path.join(path2data,folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos(root_dir, target_folder):\n",
    "    target_folder_dir = os.path.join(root_dir, target_folder)\n",
    "    classes = sorted(os.listdir(target_folder_dir))\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for cat in classes:\n",
    "\n",
    "        cat_dir = os.path.join(target_folder_dir, cat)\n",
    "        video_path = [os.path.join(cat_dir, loc) for loc in sorted(os.listdir(cat_dir))]\n",
    "        ids.extend(video_path)\n",
    "        labels.extend([cat]*len(video_path))\n",
    "    return ids, labels, classes\n",
    "def _find_classes(root_dir):\n",
    "    classes = [d.name for d in os.scandir(root_dir) if d.is_dir()]\n",
    "    classes.sort()\n",
    "    class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, class_to_idx = _find_classes(os.path.join(path2data,folder_jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, labels ,classes = get_videos(path2data, folder_jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates VideoDataset to be loaded \n",
    "    \n",
    "    Parameters:\n",
    "        root_dir\n",
    "        split - train/val\n",
    "        num_classes\n",
    "        transform\n",
    "        \n",
    "    __getitem__(idx)\n",
    "    \n",
    "    Parameters:\n",
    "        idx\n",
    "    \n",
    "    returns:\n",
    "        a list of 16 transformed(frames) and its corresponding label\n",
    "    \n",
    "    get_videos(root_dir,target_folder)\n",
    "    \n",
    "    Parameters:\n",
    "        root_dir\n",
    "        target_folder - train/val\n",
    "    \n",
    "    returns:\n",
    "        all video path, corresponding label and classes\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,root_dir, split, num_classes, transform = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.split = split\n",
    "        if transform == None:\n",
    "            self.transform = transforms.Compose([\n",
    "                    transforms.Resize((112,112), interpolation=3),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                        ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        \n",
    "        all_ids, all_labels, _ = self.get_videos(root_dir, split)\n",
    "        _, self.labels_dict = self._find_classes(os.path.join(root_dir, split))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.unique_ids = [id_ for id_, label in zip(all_ids, all_labels) \n",
    "                                    if self.labels_dict[label]<self.num_classes]\n",
    "        self.unique_labels = [label for id_, label in zip(all_ids, all_labels) \n",
    "                                            if self.labels_dict[label]<self.num_classes]  \n",
    "        \n",
    "        self.classes = list(np.unique(self.unique_labels))\n",
    "    def __getitem__(self,idx):\n",
    "        frames = []\n",
    "      \n",
    "        \n",
    "        if self.transform:\n",
    "            for frame_name in os.listdir(self.unique_ids[idx])[:16]:\n",
    "                img = Image.open(os.path.join(self.unique_ids[idx], frame_name))\n",
    "                img = self.transform(img)\n",
    "                frames.append(img)\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for frame_name in os.listdir(self.unique_ids[idx])[:16]:\n",
    "                img = Image.open(os.path.join(self.unique_ids[idx], frame_name))\n",
    "                frames.append(img)\n",
    "        \n",
    "        label= self.labels_dict[self.unique_labels[idx]]\n",
    "        frames_tr = torch.stack(frames)\n",
    "        return frames_tr, label        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.unique_ids)        \n",
    "    \n",
    "    \n",
    "    def get_videos(self,root_dir, target_folder):\n",
    "        target_folder_dir = os.path.join(root_dir, target_folder)\n",
    "        classes = sorted(os.listdir(target_folder_dir))\n",
    "        labels = []\n",
    "        ids = []\n",
    "\n",
    "        for cat in classes:\n",
    "\n",
    "            cat_dir = os.path.join(target_folder_dir, cat)\n",
    "            video_path = [os.path.join(cat_dir, loc) for loc in sorted(os.listdir(cat_dir))]\n",
    "            ids.extend(video_path)\n",
    "            labels.extend([cat]*len(video_path))\n",
    "        return ids, labels, classes\n",
    "    \n",
    "    def _find_classes(self, root_dir):\n",
    "        classes = [d.name for d in os.scandir(root_dir) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
    "        return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2data = \"C:\\\\Users\\\\USER\\\\Desktop\\\\Video_data\\\\hmdb51\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((112,112), interpolation=3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((112,112), interpolation=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_r3d_18(batch):\n",
    "    imgs_batch, label_batch = list(zip(*batch))\n",
    "    imgs_batch = [imgs for imgs in imgs_batch if len(imgs)>0]\n",
    "    label_batch = [torch.tensor(l) for l, imgs in zip(label_batch, imgs_batch) if len(imgs)>0]\n",
    "    imgs_tensor = torch.stack(imgs_batch)\n",
    "    imgs_tensor = torch.transpose(imgs_tensor, 2, 1)\n",
    "    labels_tensor = torch.stack(label_batch)\n",
    "    return imgs_tensor,labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = {x: VideoDataset(path2data, x, 4, transform = data_transform[x]) for x in [\"train\", \"val\"]}\n",
    "data_loader = {x: torch.utils.data.DataLoader(video_data[x], batch_size = 4, shuffle = True,\n",
    "                                             collate_fn = collate_fn_r3d_18) for x in [\"train\", \"val\"]}\n",
    "dataset_sizes = {x: len(video_data[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(video_data[\"train\"].classes)\n",
    "model = models.video.r3d_18(pretrained=True, progress=False)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.0501 Acc: 0.5204\n",
      "val Loss: 0.1784 Acc: 0.9710\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4003 Acc: 0.8439\n",
      "val Loss: 0.1409 Acc: 0.9565\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2711 Acc: 0.9219\n",
      "val Loss: 0.1277 Acc: 0.9565\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3177 Acc: 0.8922\n",
      "val Loss: 0.0726 Acc: 0.9855\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.3067 Acc: 0.8773\n",
      "val Loss: 0.0775 Acc: 0.9565\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1346 Acc: 0.9703\n",
      "val Loss: 0.0906 Acc: 0.9710\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1278 Acc: 0.9591\n",
      "val Loss: 0.0802 Acc: 0.9710\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1576 Acc: 0.9628\n",
      "val Loss: 0.0828 Acc: 0.9710\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1320 Acc: 0.9665\n",
      "val Loss: 0.1350 Acc: 0.9710\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1621 Acc: 0.9368\n",
      "val Loss: 0.1061 Acc: 0.9710\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1294 Acc: 0.9480\n",
      "val Loss: 0.1108 Acc: 0.9710\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1731 Acc: 0.9517\n",
      "val Loss: 0.0708 Acc: 0.9710\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0716 Acc: 0.9851\n",
      "val Loss: 0.1022 Acc: 0.9710\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0911 Acc: 0.9814\n",
      "val Loss: 0.0818 Acc: 0.9710\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1184 Acc: 0.9703\n",
      "val Loss: 0.0896 Acc: 0.9710\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1180 Acc: 0.9703\n",
      "val Loss: 0.0821 Acc: 0.9710\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1094 Acc: 0.9703\n",
      "val Loss: 0.1037 Acc: 0.9710\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0846 Acc: 0.9665\n",
      "val Loss: 0.1332 Acc: 0.9710\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0729 Acc: 0.9851\n",
      "val Loss: 0.1471 Acc: 0.9710\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0950 Acc: 0.9703\n",
      "val Loss: 0.1273 Acc: 0.9710\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0958 Acc: 0.9740\n",
      "val Loss: 0.0759 Acc: 0.9710\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0526 Acc: 0.9926\n",
      "val Loss: 0.1319 Acc: 0.9710\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0414 Acc: 0.9888\n",
      "val Loss: 0.1217 Acc: 0.9710\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0396 Acc: 0.9888\n",
      "val Loss: 0.1460 Acc: 0.9710\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0686 Acc: 0.9777\n",
      "val Loss: 0.1620 Acc: 0.9710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, loss_history, metric_history = train_model(model, criterion, optimizer, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your model \n",
    "\n",
    "<i> packages needed </i> <br>\n",
    "<i> - cv2 </i> <br>\n",
    "<i> - collections </i> <br>\n",
    "<i> - time </i>\n",
    "\n",
    "## Pipeline \n",
    "\n",
    "1) Load the video\n",
    "\n",
    "2) Create a deque object with max length of 16 frames\n",
    "\n",
    "3) Run the video and load the frames into deque object\n",
    "\n",
    "4) When deque object is filled, transform it into the appropriate Tensor dimension for the model to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"VideoClass.pth\")\n",
    "model = models.video.r3d_18(pretrained=True, progress=False)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 4) \n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2Video = \"C:\\\\Users\\\\USER\\\\Desktop\\\\Video_data\\\\hmdb51_org\\\\catch\\\\Frisbee_catch_f_cm_np1_ri_med_1.avi\"\n",
    "cap = cv2.VideoCapture(path2Video)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((112,112), interpolation=3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "frames = deque(maxlen = 16)\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "    # Our operations on the frame come here\n",
    "        time.sleep(1/fps)\n",
    "        img = Image.fromarray(frame)\n",
    "        img_tensor = transform(img)\n",
    "        frames.append(img_tensor)\n",
    "\n",
    "        if len(frames) == 16:\n",
    "            frames_tensor = torch.stack(list(frames))\n",
    "            frames_tensor = torch.transpose(frames_tensor, 1, 0)\n",
    "            frames_tensor = frames_tensor.unsqueeze(0)\n",
    "            output = model(frames_tensor.to(device))\n",
    "            action = video_data[\"train\"].classes[torch.argmax(output,dim = 1).cpu().item()]\n",
    "        \n",
    "            frame = cv2.putText(frame,action,(20,20), cv2.FONT_HERSHEY_SIMPLEX ,  \n",
    "                   1, (255,0,0), 1, cv2.LINE_AA) \n",
    "        \n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs = 25):\n",
    "    since = time.time()\n",
    "    loss_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    metric_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in data_loader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            loss_history[phase].append(epoch_loss)\n",
    "            metric_history[phase].append(1.0 - epoch_acc)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
